{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f182a15c-f1be-4c58-b23c-a41bd07282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb1bf52-bef1-4c91-9208-6c4d0094beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3ce9f5a-6fe4-4011-b7dc-5e04e4b3fbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfca1b59-f9f2-490d-ad04-c5482e8f897e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9870d8ad-6150-4594-92a6-104de0472162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25061aba-e19a-46ba-97c5-067f88392bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"target\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d9a28b9-cb15-45ea-9d3d-a2c31240bda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"target\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63e23763-2fdf-46bb-bc76-8bb1cfb38696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"target\"] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afac94a9-bf2a-4383-8cd1-d3ec33af3385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"target\"] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6335b9ed-1ab9-4608-a4c3-025d6d6a6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[790000:-790000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f28c468-8fea-4057-a3b6-7dba8956a9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"target\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caf190ab-cbfd-4eb2-8a34-a7d868f3ed65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80002449-4553-457f-b9f7-027f0f69912f",
   "metadata": {},
   "source": [
    "# preprocessing start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "905213c5-2ad5-45f9-abd0-0b517b12c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/macbook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbook/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Define preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation and special characters\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3. Stopword Removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 4. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e1b8de2-fe57-415d-8e43-3f195cd63891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pr-text\"]=data[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66490b42-a977-40b9-ab0d-d1cc8c0e13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to processed_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the processed dataset to a new CSV file\n",
    "output_file_path = \"processed_tweets.csv\"\n",
    "data.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Processed dataset saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12ffd0a-794b-4d20-ad12-ed256ef58f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"processed_tweets.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b7d26c3-340b-4293-80f1-3b69054803b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data [[\"pr-text\",\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fe39eab-4475-4b3c-9fe3-ae9d4b8c6445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr-text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sherilynmoon unfortun signal tweet photo phone...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drop car get brake look hope doesnt cost insan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>colin kelli clyde1 get one republ neyo im scho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>littl upset chanc take 4 5 realli cool pictur ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batheaston bypass flow better london road stil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pr-text  target\n",
       "0  sherilynmoon unfortun signal tweet photo phone...       0\n",
       "1  drop car get brake look hope doesnt cost insan...       0\n",
       "2  colin kelli clyde1 get one republ neyo im scho...       0\n",
       "3  littl upset chanc take 4 5 realli cool pictur ...       0\n",
       "4  batheaston bypass flow better london road stil...       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbaf32e-9f52-4986-94ea-fdeee38ed8ba",
   "metadata": {},
   "source": [
    "# indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c84d2f0-eec1-4205-9da5-f970d6edb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherilynmoon: [0]\n",
      "wale: [0, 7219, 12108, 13945]\n",
      "photo: [0, 217, 329, 597, 1579, 1860, 2950, 3581, 3637, 3991, 4095, 4575, 5550, 5601, 5656, 5734, 6039, 6340, 6530, 7649, 8084, 9325, 10165, 10325, 10426, 10649, 10706, 11279, 11410, 11842, 11963, 12179, 12495, 13736, 13762, 13887, 14004, 14142, 14188, 14352, 14558, 14613, 14745, 14813, 14867, 14939, 14972, 15885, 15888, 16444, 16472, 16579, 16876, 16881, 16973, 17256, 17483, 17555, 18179, 18441, 18611, 18699, 18875, 18902, 18970, 19239, 19665]\n",
      "signal: [0, 3366, 5549, 6276, 6587, 8875, 9634, 11024, 13667, 16806]\n",
      "unfortun: [0, 49, 322, 855, 940, 1041, 1228, 1231, 1437, 1489, 1816, 2078, 2460, 3269, 3427, 3665, 3922, 4437, 4473, 4482, 4503, 4601, 4638, 4677, 4848, 5672, 5907, 5995, 6041, 6106, 6865, 7965, 9254, 9356, 11501, 11515, 12251, 12776, 14583, 19148, 19926]\n",
      "phone: [0, 3, 40, 42, 107, 202, 296, 409, 432, 684, 732, 769, 990, 1105, 1215, 1306, 1324, 1555, 1659, 1687, 1721, 1826, 2020, 2108, 2125, 2213, 2314, 2322, 2330, 2389, 2425, 2471, 2609, 2655, 2809, 2841, 2865, 2872, 3016, 3023, 3253, 3417, 3579, 3600, 3641, 3765, 3949, 4025, 4274, 4311, 4324, 4339, 4341, 4368, 4467, 4576, 4608, 4657, 4870, 4881, 5059, 5105, 5259, 5378, 5381, 5393, 5497, 5546, 5611, 5640, 5749, 5758, 5815, 6050, 6055, 6123, 6280, 6493, 6530, 6690, 6787, 6812, 6850, 6968, 7115, 7127, 7361, 7382, 7384, 7431, 7456, 7628, 7632, 7642, 7682, 7706, 7794, 7805, 7856, 8383, 8398, 8420, 8605, 8888, 8910, 9264, 9293, 9336, 9361, 9481, 9487, 9763, 9920, 10041, 10306, 10572, 10982, 11225, 11240, 11421, 12000, 12399, 12440, 13052, 13085, 13138, 13402, 13992, 14115, 14212, 14214, 14309, 14410, 14562, 14672, 14913, 14929, 15409, 15512, 15550, 16182, 16187, 16263, 16404, 16612, 16807, 16925, 17253, 17522, 17653, 17737, 17755, 18204, 18236, 18329, 18893, 19388, 19531, 19640, 19961]\n",
      "tweet: [0, 49, 91, 102, 229, 409, 419, 528, 571, 596, 614, 647, 727, 1067, 1132, 1174, 1234, 1324, 1378, 1461, 1597, 1864, 1978, 1990, 2020, 2058, 2146, 2179, 2185, 2682, 2697, 2768, 2775, 3006, 3085, 3312, 3530, 3657, 3862, 3980, 3995, 4023, 4447, 4852, 4861, 4974, 5071, 5221, 5693, 5782, 5858, 6385, 6428, 6475, 6508, 6690, 6927, 7126, 7206, 7252, 7310, 7438, 7496, 7530, 7659, 7841, 7865, 7955, 8261, 8378, 8593, 8722, 8920, 8983, 9031, 9386, 9553, 9798, 9862, 9950, 9991, 10096, 10155, 10279, 10294, 10299, 10385, 10483, 10643, 10656, 10697, 10726, 10731, 10914, 10924, 10935, 10943, 11034, 11042, 11104, 11164, 11394, 11415, 11455, 11477, 11763, 11910, 11932, 12048, 12055, 12063, 12168, 12296, 12363, 12390, 12478, 12577, 12803, 12816, 12842, 13071, 13081, 13085, 13130, 13182, 13331, 13422, 13432, 13446, 13449, 13660, 13687, 13766, 13866, 13874, 13875, 13877, 13899, 13911, 13968, 14000, 14127, 14205, 14294, 14305, 14323, 14325, 14529, 14612, 14621, 14678, 14746, 14917, 15064, 15071, 15102, 15301, 15306, 15312, 15383, 15403, 15446, 15486, 15656, 15755, 15763, 15770, 15875, 15921, 15927, 15977, 15982, 16008, 16046, 16099, 16368, 16518, 16659, 16668, 16704, 16710, 16776, 16781, 16804, 16968, 17042, 17063, 17404, 17429, 17431, 17532, 17587, 17721, 17738, 17795, 17861, 17871, 17894, 17916, 17917, 17977, 17986, 17995, 18109, 18143, 18217, 18262, 18274, 18290, 18436, 18462, 18549, 18600, 18675, 18769, 18785, 18804, 18847, 18855, 18954, 18965, 18978, 19008, 19034, 19144, 19149, 19332, 19343, 19411, 19450, 19525, 19612, 19709, 19907, 19976, 19981, 19997]\n",
      "whilst: [0, 841, 13404, 14181, 16594]\n",
      "drop: [1, 744, 836, 898, 932, 1133, 1397, 1433, 2520, 2972, 3052, 3641, 3948, 4036, 4341, 4420, 4960, 5419, 5523, 6457, 6549, 6632, 7137, 7362, 8101, 8261, 8277, 9210, 10816, 10830, 10855, 11981, 13480, 14369, 14951, 14986, 15494, 17673, 18434, 18764, 19045, 19747]\n",
      "cost: [1, 926, 1723, 1913, 1987, 2104, 2467, 4577, 4593, 6004, 6379, 16222, 16616, 17326, 19077]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "data['pr-text'] = data['pr-text'].astype(str)\n",
    "\n",
    "def build_inverted_index(data):\n",
    "    inverted_index = defaultdict(list)  # Dictionary with term as key, list of doc IDs as value\n",
    "    \n",
    "    for doc_id, text in enumerate(data['pr-text']):\n",
    "        tokens = text.split()  # Split preprocessed text into tokens\n",
    "        for token in set(tokens):  # Use set to avoid duplicate entries for the same doc\n",
    "            inverted_index[token].append(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "# Build the inverted index\n",
    "inverted_index = build_inverted_index(data)\n",
    "\n",
    "# Display a sample of the index\n",
    "for term, doc_ids in list(inverted_index.items())[:10]:  # Show first 10 terms\n",
    "    print(f\"{term}: {doc_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307a311-5ea1-4352-ad55-b9d72ca23927",
   "metadata": {},
   "source": [
    "# tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a874e8d-ec7e-44b6-be14-12da8549a510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             pr-text  \\\n",
      "0  sherilynmoon unfortun signal tweet photo phone...   \n",
      "1  drop car get brake look hope doesnt cost insan...   \n",
      "2  colin kelli clyde1 get one republ neyo im scho...   \n",
      "3  littl upset chanc take 4 5 realli cool pictur ...   \n",
      "4  batheaston bypass flow better london road stil...   \n",
      "\n",
      "                                              tf_idf  \n",
      "0  {'sherilynmoon': 1.2379296939107607, 'unfortun...  \n",
      "1  {'drop': 0.5605243575457017, 'car': 0.46816794...  \n",
      "2  {'colin': 0.9210290370726142, 'kelli': 0.75055...  \n",
      "3  {'littl': 0.610723676286357, 'upset': 0.402149...  \n",
      "4  {'batheaston': 1.1003819501428984, 'bypass': 1...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tf_idf(data):\n",
    "    documents = data['pr-text']\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    # 1. Calculate Term Frequency (TF)\n",
    "    tf = []  # List of dictionaries for each document\n",
    "    for text in documents:\n",
    "        tokens = text.split()\n",
    "        total_terms = len(tokens)\n",
    "        term_count = Counter(tokens)\n",
    "        tf.append({term: count / total_terms for term, count in term_count.items()})\n",
    "    \n",
    "    # 2. Calculate Document Frequency (DF)\n",
    "    df = {}\n",
    "    for text in documents:\n",
    "        unique_terms = set(text.split())\n",
    "        for term in unique_terms:\n",
    "            df[term] = df.get(term, 0) + 1\n",
    "    \n",
    "    # 3. Calculate Inverse Document Frequency (IDF)\n",
    "    idf = {term: math.log(total_docs / df[term]) for term in df}\n",
    "    \n",
    "    # 4. Calculate TF-IDF\n",
    "    tf_idf = []\n",
    "    for doc_tf in tf:\n",
    "        doc_tf_idf = {term: doc_tf[term] * idf[term] for term in doc_tf}\n",
    "        tf_idf.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "# Calculate TF-IDF\n",
    "data['tf_idf'] = calculate_tf_idf(data)\n",
    "\n",
    "# View the result\n",
    "print(data[['pr-text', 'tf_idf']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa42fcf7-1a2e-4f93-a253-4487b8fde1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features to the number of features you want\n",
    "\n",
    "# Fit the TF-IDF Vectorizer on the processed text and transform the text to a feature matrix\n",
    "X = tfidf_vectorizer.fit_transform(data['pr-text']).toarray()\n",
    "y=data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d51a2be3-7187-4257-b11c-bfc5cb0c5475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      2018\n",
      "           4       0.74      0.78      0.76      1982\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.76      0.76      0.76      4000\n",
      "weighted avg       0.76      0.76      0.76      4000\n",
      "\n",
      "Accuracy: 0.7565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear')  # Using linear kernel, you can experiment with other kernels\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f65d88-9e2b-4baa-9881-3eca889f6058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
